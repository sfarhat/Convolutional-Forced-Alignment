import torch
from ctcdecode import CTCBeamDecoder
from grad_cam import Sequential_GRAD_CAM
import matplotlib.pyplot as plt

def test(model, test_loader, criterion, device, transformer):
    """
    Evaluation for model.

    Args:
        model (nn.Module): Network to test on
        test_loader (torch.utils.data.dataloader): DataLoader for test dataset
        criterion (nn.modules.loss): Loss function
        device (torch.device): Device (cpu or cuda)
        transformer (timit_utils.PhonemeTransformer or utils.TextTransfomer): Transformer that handles all labels <-> text
    """

    model.eval()
    with torch.no_grad():
        phon_err_rates = []
        for inputs, input_lengths, targets, target_lengths in test_loader:

            inputs, targets = inputs.to(device), targets.to(device)
            # output of shape batch x time x classes
            output = model(inputs)
            output = output.transpose(0, 1)
            loss = criterion(output, targets, input_lengths, target_lengths)
            
            # Transpose back so that we can iterate over batch dimension
            output = output.transpose(0, 1)

            for log_probs, true_target, target_len, input in zip(output, targets, target_lengths, inputs):

                # For TIMIT, moving to 39 test labels occurs in target_to_text()
                guessed_text = timit_decode(log_probs, target_len, transformer)
                true_text = transformer.target_to_text(true_target[:target_len])

                per = phoneme_error_rate(guessed_text, true_text).item()
                phon_err_rates.append(per)

    print('Average PER: {}%'.format(sum(phon_err_rates) / len(phon_err_rates) * 100))

def show_activation_map(model, device, input, desired_phone_idx):
    """Creates and displays an activation map using GRAD-CAM on top of an input spectrogram. Given that in our model the time dimension exists, a target class (desired phoneme)
    can span multiple time-steps, so when asking for a map wrt a class, the specific phoneme must be provided in the form of its index in the (non-artificially-repeating) original phoneme transcript. 
    In turn, a map will be generated by taking the element-wise maximum among all maps corresponding to the desired phoneme.

    For example, if the non-duration-including phoneme transcript is ['sil', 'ae', 's', 'ae'], then providing desired_phone_idx = 1 will generate a CAM 
    across all timesteps covered by the first 'ae'.

    Args:
        model (nn.Module): Network to put input into
        device (torch.device): Device (cpu or cuda) 
        input (Tensor): Input to be fed into model of shape channel x features x time
        desired_phone_idx (int): The index of the desired phoneme in the output transcript to generate the activation map with respect to 
    """

    gcam = Sequential_GRAD_CAM(model)

    # Artifically create batch dimension
    input = input.to(device).unsqueeze(0)
    # output of shape batch x time x classes
    log_probs = model(input).squeeze(0)

    guessed_labels = torch.argmax(log_probs, dim=1)

    # guessed_indices don't requires_grad, so we index into log_probs to get target_classes
    target_classes, target_classes_start, target_classes_end = gcam.get_target_classes(log_probs, guessed_labels, desired_phone_idx)
    # Squeeze out batch and channel dimensions when providing interpolation size
    cam = gcam.generate_cam(input.squeeze(0).squeeze(0).shape, target_classes, target_classes_start, target_classes_end)

    plotted_cam = cam.squeeze(0).permute(1, 2, 0).cpu()
    plotted_input = input.squeeze(0).permute(1, 2, 0).cpu()
    plt.imshow(plotted_cam, alpha=1, cmap='jet')
    plt.imshow(plotted_input, alpha=0.5, cmap='binary')
    plt.savefig('cam.png')
    plt.show()

    # TODO: if we want to do something wih this + forced alignment, use time transformation function to go back to waveform time

def timit_decode(log_probs, target_len, transformer):
    """Generates 39-label phoneme sequence from output of network for a single sample"""

    phon_indices = torch.argmax(log_probs, dim=1)
    return transformer.target_to_text(phon_indices[:target_len])

def phoneme_error_rate(guess, truth):
    """Phoneme Error Rate of sequence"""

    collapsed_guess, collapsed_true = collapse_repeats(guess), collapse_repeats(truth)
    levenshtein_dist = edit_distance(collapsed_guess, collapsed_true)
    
    return levenshtein_dist / len(truth)

def collapse_repeats(sequence):
    """Collapse repeats from sequence to be used for PER"""

    result = []
    prev = None

    for x in sequence:
        if x == prev:
            continue

        result.append(x)
        prev = x

    return result

def edit_distance(a, b):
    """Levenshtein Distance"""

    # add 1 for blank beginning
    m, n = len(a)+1, len(b)+1
    d = torch.empty(m, n)

    for i in range(m):
        d[i, 0] = i

    for j in range(n):
        d[0, j] = j

    for i in range(1, m):
        for j in range(1, n):
            # off-by-one for first char not starting at index 0 of matrix
            if a[i-1] == b[j-1]:
                sub = 0
            else:
                sub = 1
            d[i, j] = min(d[i-1, j] + 1,
                        d[i, j-1] + 1,
                        d[i-1, j-1] + sub)

    return d[m-1, n-1]

def beam_search_decode(log_probs, transformer):

    # Using this ctc decoder: https://github.com/parlance/ctcdecode
    # Labels come from order specified in utils.py, _ represents blank
    labels = list("_ abcdefghijklmnopqrstuvwxyz'")

    decoder = CTCBeamDecoder(
        labels,
        model_path=None,
        alpha=0,
        beta=0,
        cutoff_top_n=40,
        cutoff_prob=1.0,
        beam_width=100,
        num_processes=16,
        blank_id=0,
        log_probs_input=True
    )

    # input to decoder needs to be of shape BATCHSIZE x N_TIMESTEPS x N_LABELS
    # Currently doing single samples, so unsqueeze to create batch of 1
    beam_results, beam_scores, timesteps, out_lens = decoder.decode(log_probs.unsqueeze(dim=0))
    # beam_results is of shape (num_batches, num_beams, time), so to get top beam, index [0][0]
    # cut it off by the appropriate length out_lens with same index
    return transformer.target_to_text(beam_results[0][0][:out_lens[0][0]])


def greedy_decode(log_probs, transformer):

    char_indices = torch.argmax(log_probs, dim=1)
    transcript = []
    blank_label = 0
    prev = None

    for idx in range(len(char_indices)):
        char = char_indices[idx].item()
        if char != blank_label:
            if char != prev:
                transcript.append(char)
        prev = char

    return transformer.target_to_text(transcript)

